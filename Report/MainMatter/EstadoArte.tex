 
%===================================================================================
% Chapter: Estado del Arte
%===================================================================================
\chapter{Estado del Arte}\label{chapter:estadoarte}
%===================================================================================

El campo del \textit{image captioning} ha evolucionado significativamente en la última década, impulsado por el avance de modelos de aprendizaje profundo. En este capítulo, se presentan los principales enfoques y modelos que han marcado hitos en esta área, destacando sus arquitecturas, metodologías y contribuciones.

\section{Modelos Basados en Redes Recurrentes}

Los primeros avances en generación de descripciones de imágenes se apoyaron en arquitecturas \textit{encoder-decoder} con redes neuronales recurrentes (RNN). Uno de los primeros modelos destacados fue \textbf{Show and Tell} \cite{vinyals2015show}, que propuso un enfoque generativo basado en una arquitectura recurrente profunda. Este modelo utilizó una red convolucional (CNN) para la extracción de características visuales y una red LSTM para la generación de texto. Fue entrenado para maximizar la probabilidad de generar una descripción textual dada una imagen, aprendiendo únicamente a partir de descripciones de imágenes.

Posteriormente, \textbf{Show, Attend and Tell} \cite{xu2015show} introdujo mecanismos de atención visual, permitiendo que el modelo enfocara diferentes regiones de la imagen en cada paso de generación. Este enfoque mejoró la calidad de las descripciones y presentó una formulación matemática más avanzada para el cálculo de la atención.

\section{Modelos Basados en Redes Convolucionales}

En un intento por superar las limitaciones de las RNN, \textbf{Convolutional Image Captioning} \cite{aneja2018convcap} propuso una arquitectura basada en CNNs para la generación de texto. Este modelo demostró que las CNNs pueden superar a las LSTM en tareas de \textit{captioning}, especialmente cuando se combinan con mecanismos de atención, mitigando problemas como el desvanecimiento del gradiente. Se realizó un análisis detallado que proporcionó razones convincentes para preferir los enfoques de generación de lenguaje convolucional. 

\section{Atención y Modelos Jerárquicos}

Otro enfoque relevante fue el modelo \textbf{Bottom-Up and Top-Down} \cite{anderson2018bottom}, que implementó un mecanismo de atención jerárquico basado en la segmentación de objetos dentro de la imagen. Este modelo propuso una estrategia combinada de atención Bottom-Up y Top-Down, permitiendo un análisis más profundo de la imagen a través de múltiples pasos de razonamiento. En el mecanismo Bottom-Up, basado en Faster R-CNN, se proponen regiones de la imagen, cada una con un vector de características asociado. Por otro lado, el mecanismo Top-Down determina las ponderaciones de estas características, lo que permite una atención más precisa a nivel de objetos y regiones destacadas.

\textbf{Knowing When to Look} \cite{lu2017knowing} propuso un mecanismo de atención adaptativo mediante un centinela visual, que decide cuándo prestar atención a la imagen y cuándo confiar en el contexto textual generado previamente. Este modelo abordó una limitación clave de los enfoques tradicionales de atención, que forzaban la atención visual en cada palabra generada, incluso cuando no era necesaria. En lugar de ello, el modelo introdujo un centinela visual que, en cada paso de tiempo, determina si es necesario extraer información de la imagen y, de ser así, selecciona las regiones relevantes. Esto permite al decodificador alternar de manera inteligente entre la información visual y el contexto lingüístico, mejorando la precisión y fluidez de las descripciones generadas.

\section{Transformers y Modelos Multimodales}

Con la llegada de los Transformers, los modelos de \textit{captioning} han adoptado arquitecturas más avanzadas. \textbf{Multimodal Transformer} \cite{yu2019multimodal} exploró la representación visual multi-vista para mejorar la generación de texto, ampliando el éxito del modelo Transformer en traducción automática a la tarea de subtítulos de imágenes. A diferencia de los enfoques tradicionales basados en codificador-decodificador, que utilizan una CNN para extraer características visuales y una RNN con mecanismos de atención para generar texto, este modelo propone un bloque de atención unificado que captura simultáneamente interacciones intra e intermodales. Esto permite un razonamiento multimodal más complejo, integrando tanto la autoatención (interacciones intramodales) como la coatención (interacciones intermodales) en una arquitectura modular y profunda.

\textbf{ViT} \cite{dosovitskiy2021image} aplicó Transformers directamente a imágenes dividiéndolas en secuencias de parches, lo que representó un cambio significativo en el procesamiento de información visual. A diferencia de los enfoques anteriores, que combinaban Transformers con redes convolucionales (CNNs) o reemplazaban solo ciertos componentes de las CNNs, ViT demostró que una arquitectura basada únicamente en Transformers puede lograr un rendimiento excepcional en tareas de clasificación de imágenes. Este enfoque elimina la dependencia de las CNNs, procesando las imágenes como secuencias de parches lineales y aplicando mecanismos de atención pura para capturar relaciones globales entre ellos.

\textbf{CLIP} \cite{radford2021learning} introdujo el aprendizaje multimodal a gran escala utilizando correspondencias entre imágenes y texto en internet. A diferencia de los sistemas de visión por computadora tradicionales, que se entrenan para predecir un conjunto fijo de categorías de objetos, CLIP aprende representaciones visuales directamente a partir de texto sin procesar, lo que le permite capturar una gama más amplia de conceptos visuales. Este enfoque se basa en una tarea de preentrenamiento simple pero efectiva: predecir qué descripción de texto corresponde a una imagen dada.

\textbf{BLIP} \cite{li2022blip} amplió la capacidad de los modelos al abordar tanto la generación como la comprensión de imágenes y videos, logrando mejoras significativas en una amplia gama de tareas de visión y lenguaje. A diferencia de los modelos preentrenados existentes, que suelen especializarse en tareas de comprensión o generación, BLIP propone un marco de preentrenamiento unificado que se transfiere de manera flexible a ambas. 

\section{Resumen y Tendencias Actuales}

La evolución del \textit{image captioning} ha pasado de modelos basados en RNNs con atención visual a enfoques más sofisticados que integran Transformers y aprendizaje multimodal. Modelos recientes como BLIP y CLIP han demostrado que la combinación de visión y lenguaje en grandes volúmenes de datos puede llevar a mejoras sustanciales en la generación y comprensión de imágenes. 

Las tendencias actuales apuntan a modelos más eficientes y escalables, con capacidades mejoradas en la generación de texto y una mayor comprensión del contexto visual. El impacto de estas tecnologías se extiende más allá del \textit{image captioning}, beneficiando tareas como búsqueda visual, generación de contenido y asistencia en accesibilidad.

