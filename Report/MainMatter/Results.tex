%===================================================================================
% Chapter: Experimentación y resultados
%===================================================================================
\chapter{Experimentación y resultados}\label{chapter:resultados}
%===================================================================================

Para evaluar el modelo de subtitulado de imágenes, se seleccionó un conjunto de 391 imágenes de diversas colecciones, garantizando una  representación variada de los datos.

Una vez que las imágenes fueron seleccionadas, se crearon textos descriptivos para cada una de ellas, las cuales fueron recopiladas en un documento que se utilizó como conjunto de referencia para la evaluación.

Seguidamente, se utilizó el modelo propuesto para generar descripciones automáticas para las imágenes, que se compararon directamente con las descripciones generadas manualmente mediante las métricas de evaluación BLEU, METEOR, CIDEr y SPICE, ROUGE-1 y ROUGE-L.

\subsection*{Métricas empleadas}
\begin{itemize}
    \item \textbf{CIDEr}: es una forma de evaluar la calidad de las descripciones textuales generadas. Mide la similitud entre una descripción generada y la de referencia. Se basa en la idea de que las buenas descripciones no solo deben ser similares a las descripciones de referencia en términos de elección de palabras y gramática, sino también en términos de significado y contenido.
    
    \item \textbf{SPICE}: mide la calidad semántica de las descripciones de imágenes generadas por modelos. A diferencia de otras métricas que se centran en la similitud superficial de palabras, evalúa el contenido semántico dividiendo las descripciones en representaciones de gráficos semánticos, las cuales incluyen objetos, atributos y relaciones. Estos se comparan luego con gráficos de referencia creados a partir de descripciones humanas. Esta métrica se enfoca en capturar la precisión y la integridad semántica, asegurándose de que la descripción generada refleje correctamente los elementos y las relaciones presentes en la imagen.

    \item \textbf{BLEU}: se calcula comparando el texto traducido automáticamente con uno o más textos traducidos por humanos. Se considera que la calidad es la correspondencia entre la salida de la máquina y la de una persona.
    
    \item \textbf{METEOR}: tiene en cuenta tanto la precisión como la fluidez de la traducción, así como el orden en que aparecen las palabras. El algoritmo compara el texto traducido con la traducción de referencia humana descomponiéndolos en fragmentos y calculando la similitud entre cada fragmento. Finalmente, se utiliza el promedio ponderado de estas medidas para calcular el puntaje METEOR general.
    
    \item \textbf{ROUGE}: es un conjunto de métricas utilizadas para evaluar la calidad de los modelos de traducción y resumen de documentos. Mide la superposición entre un resumen o traducción generado por el sistema y un conjunto de resúmenes o traducciones de referencia creados por humanos.
    \item \textbf{ROUGE-N}: mide la superposición de n-gramas entre los resúmenes generados por el modelo y los resúmenes de referencia.
    \item \textbf{ROUGE-L}: Se basa en la longitud de la subsecuencia común más larga. Calcula la media armónica ponderada, combinando el puntaje de precisión y el de recall. No requiere coincidencias consecutivas, sino coincidencias en secuencia.
\end{itemize}

La siguiente tabla representa los resultados obtenidos al evaluar las métricas:

\begin{table}[h!]
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Métricas & SPICE & CIDEr & METEOR & BLEU & ROUGE-1 & ROUGE-L \\ \hline
    Resultados & 0.0469 & 0.3927 & 0.2452 & 0.110 & 0.2792 & 0.2506 \\ \hline
    \end{tabular}
\end{table}

\subsection*{Valoracíon de los resultados}
    Los resultados indican que el modelo genera descripciones con cierta coherencia léxica pero baja precisión semántica, como lo reflejan los puntajes bajos en SPICE (0.0469) y CIDEr (0.3927), lo que sugiere dificultades para capturar correctamente la estructura semántica y los términos clave de la imagen. METEOR (0.2452) y ROUGE (0.2792 y 0.2506) muestran una superposición moderada con las descripciones humanas, mientras que BLEU (0.0110) es extremadamente bajo, indicando que la redacción del modelo difiere significativamente de las referencias. Sin embargo, si las descripciones humanas son inconsistentes o imprecisas, estas métricas pueden estar penalizando al modelo más de lo necesario.

    Como análisis adicional, se puede destacar que las descripciones generadas por el modelo BLIP, fueron seleccionadas un 88\% de las veces, demostrando una mayor preferencia hacia el mismo por el modelo de selección.