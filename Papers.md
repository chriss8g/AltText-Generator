# Enfoques Comunes
### Arquitectura CNN-RNN (Encoder-Decoder):
Show and Tell (2015)
   - Propuesta por Google.
   - Encoder: CNN (InceptionV3) para obtener un vector representativo de la imagen.
   - Decoder: LSTM para generar las descripciones palabra por palabra.
   - Referencia: [Paper](https://arxiv.org/abs/1411.4555)

Show, Attend and Tell (2015)  
   - Introdujo atenci贸n visual al modelo Show and Tell (mezclando atenci贸n con CNN-RNN).
   - Mejora la generaci贸n al identificar regiones clave en la imagen mientras genera texto.
   - Referencia: [Paper](https://arxiv.org/abs/1502.03044)

Otro ahi que parece tar duro:
[Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf)

### Modelos con Atenci贸n Visual:

Buttom up Top down
[Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf)

Otro ahi:
[Paper](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=image+captioning+using+visual+attention&btnG=#d=gs_qabs&t=1735539564573&u=%23p%3DlCmsAElTZjYJ)

### Transformers Multimodales:

[Paper](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=image+captioning+using+multimodal+transformer&oq=image+captioning+using+multimodal+transfo#d=gs_qabs&t=1735539717988&u=%23p%3Dl5eMkmLxdlwJ)


### Adicional
Ya de paso dejo aqui un papers que se supone que es una revision del estado del arte de image captioning. Le pase x arribita y lo vi bastante bien, lo que esta mas largo que el nepe de Rasputin.

[Paper](https://arxiv.org/pdf/1810.04020)